# Exercise 04 - Ingest dataset 

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex04              |
|   Files to turn in :    |                    |
|   Forbidden function :  |  None              |
|   Remarks :             |  n/a               |

Now that you know the basics of how Elasticsearch works, you are ready to work with a real dataset !!
And to make this fun you gonna use the same dataset as for the SQL day so you can see differences between SQL and NoSQL.

There are many ways you can ingest data into Elasticsearch. In the previous exercise, you've seen how to create a document manually.  
You could do this for every line of the CSV, with a python script for instance that parse the CSV and create a document for each line. There is an Elasticsearch client API for many languages that help to connect to the cluster (to avoid writing http request in python): [Elasticsearch client](https://www.elastic.co/guide/en/elasticsearch/client/index.html).

But there is an easier way: Logstash. Logstash is the ETL (Extract Transform Load, you will learn more on this in the next day) tool of the Elasticsearch stack. We don't want you to spend to much time learning how to use Logstash so we will guide you step by step:

1- Download [logstash](https://www.elastic.co/downloads/logstash)
2- Un-tar the file (still in your `/goinfre`)  
3- move the 'ingest-pipeline.conf' to the config/ in the logstash directory  

This file describes all the operations that logstash shall do to ingest the data. Let's take a look at the file:

The file is split into three parts :  
input {} --> definition of the inputs  
filter {} --> operation to perform on the inputs  
output {} --> definition of the outputs  

```
input {
	file { 
		path => "/absolute/path/to/appstore_games.csv"
		start_position => "beginning"
		sincedb_path => "sincedb_file.txt"
	}
}
```
file {} --> our input will be a file, could be something else (stdin, data stream, ...etc)  
path --> location of the input file  
start_position --> where to start reading the file  
sincedb_path --> logstash stores its position in the input file, so if new lines are added, only new line will be processed. (ie, if you want to re run the ingest, delete the sincedb_file)  

```
filter {
	csv {
		separator => ","
		columns => ["URL","ID","Name","Subtitle","Icon URL","Average User Rating","User Rating Count","Price","In-app Purchases","Description","Developer","Age Rating","Languages","Size","Primary Genre","Genres","Original Release Date","Current Version Release Date"]
		remove_field => ["message", "host", "path", "@timestamp"]
		skip_header => true
  }
  mutate {
		gsub => [ "Description", "\\n", "
"]
		gsub => [ "Description", "\\t", "	"]
		gsub => [ "Description", "\\u2022", "•"]
		gsub => [ "Description", "\\u2013", "–"]
		split => { "Genres" => "," }
		split => { "Languages" => "," }
  }
}
```
csv{} --> we use the csv pluging to parse the file  
separator --> split each line on the comma  
column --> name of the columns (will create one field in the index mapping per column)  
remove_field --> here we remove 4 fields, those 4 fields are added by logstash to the raw data but we don't need them.  
skip_header --> skip the first line  
mutate{} --> When logstash parse the field it escape any '\' it found. This changes a '\\n', '\\t', '\\u2022' and '\\u2013' into a '\\\\n', '\\\\t', '\\\\u2022', '\\\\u2013' respectively, which is not what we want. The mutate pluging is used here to fix this.  
gsub --> subtitute '\\n' by a new line and the '\\\\u20xx' by its unicode character.  
split --> split the "Genres" and "Languages" field on the "," instead of a single string like "FR, EN, KR" we will have ["EN", "FR", "KR]

```
output {
	elasticsearch {
		hosts => "http://localhost:9200"
		index => "appstore_games_tmp"
	}
	stdout {
		codec => "dots"
	}
}
```
elasticsearch {} --> we want to output to an Elasticsearch cluster  
hosts --> ip of the cluster  
index --> name of the index where to put the data (index will be created if not existing, otherwise data are added to the cluster)  
stdout {} --> we also want an output on stdout to follow the ingestion process  
codec => "dots" --> print one dot '.' for every document ingested  

So, all we do here is create one document for each line of the csv. Then, for each line, split on comma and put the value in a field of the document with the name defined in 'columns'. Exactly what you would have done with Python but in much less line of code.  
Now, let's run Logstash:
- To run logstash you will need to install a JDK or JRE. On a 42 computer, you can do this from the MSC.
- Edit the ingest-pipeline.conf with the path to the appstore_games.csv
- `./bin/logstash -f config/ingest-pipeline.conf`  

You should have 17007 documents in your index.
