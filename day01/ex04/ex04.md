# Exercise 04 - Ingest dataset 

|                         |                    |
| -----------------------:| ------------------ |
|   Turnin directory :    |  ex04              |
|   Files to turn in :    |   |
|   Forbidden function :  |  None              |
|   Remarks :             |  n/a               |

Now that you know the basis of how Elasticsearch works, you are ready to work with a real dataset !!
And to make this fun you gonna use the same dataset as for the SQL day so you can see differences between SQL and noSQL.

There are many way you can ingest data into Elasticsearch. In the previous exercise, you've seen how to create document manually.  
You could do this for every line of the csv, with a python script for instance that parse the csv and create a document for each line. There is an Elasticsearch client API for many languages that help to connect to the cluster (to avoid writting http request in python): <href src="https://www.elastic.co/guide/en/elasticsearch/client/index.html"><u><font color="blue">https://www.elastic.co/guide/en/elasticsearch/client/index.html</font></u></href>

But there is an easier way: Logstash. Logstash is the ETL (Extract Transform Load, you will learn more on this ine the next day) tool of the Elasticsearch stack. We don't want you to spend to much time learning how to use Logstash so we will guide you step by step:

1- Download logstash:  
https://www.elastic.co/downloads/logstash  
2- Un-tar the file (still in your /goinfre)  
3- move the 'ingest-pipeline.conf' to the config/ in the logstash directory  

This file describes all the operations that logstash shall do to ingest the data. Let's take a look at the file:

The file is splitted in three parts :  
input {} --> definition of the inputs  
filter {} --> operation to perform on the inputs  
output {} --> definition of the outputs  

```
input {
	file { 
		path => "/path/to/the/dataset/appstore_games.csv"
		start_position => "beginning"
		sincedb_path => "sincedb_file.txt"
	}
}
```
file {} --> our input will be a file, could be something else (stdin, data stream, ...etc)  
path --> location of the input file  
start_position --> where to start reading the file  
sincedb_path --> logstash store its position in the input file, so if new lines are added, only new line will be processed. (ie, if you want to re run the ingest, delete the sincedb_file)  

```
filter {
	csv {
		separator => ","
		columns => ["URL","ID","Name","Subtitle","Icon URL","Average User Rating","User Rating Count","Price","In-app Purchases","Description","Developer","Age Rating","Languages","Size","Primary Genre","Genres","Original Release Date","Current Version Release Date"]
		remove_field => ["message", "host", "path", "@timestamp"]
  }
}
```
csv{} --> we use the csv pluging to parse the file  
separator --> split each line on the comma  
column --> name of the columns (will create one field in the index mapping per column)  
remove_field --> here we remove 4 fields, those 4 fields are added by logstash to the raw data but we don't need them.  

```
output {
	elasticsearch {
		hosts => "http://localhost:9200"
		index => "data"
	}
	stdout {
		codec => "dots"
	}
}
```
elasticsearch {} --> we want to output to an Elasticsearch cluster  
hosts --> ip of the cluster  
index --> name of the index where to put the data (index will be created if not existing, otherwise data are added to the cluster)  
stdout {} --> we also want an output on stdout to follow the ingestion process  
codec => "dots" --> print one dot '.' for every document ingested  

So, all we do here is creating one document for each line of the csv and for each line, split on comma and put the value in a field of the document with the name defined in 'columns'. Exactly what you would have done with Python but in much less line of code.  
Now, let's run Logstash:  
`./bin/logstash -f config/ingest-pipeline.conf`  

 
You should have 17007 documents in your index.
